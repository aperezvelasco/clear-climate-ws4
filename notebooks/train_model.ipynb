{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ§  Train DeepESD for ERA5 â†’ CERRA Spatial Downscaling\n",
    "\n",
    "This notebook demonstrates how to train the DeepESD convolutional model on ERA5 (0.25Â°) to CERRA (0.05Â°) temperature data.\n",
    "\n",
    "We will:\n",
    "1. Load preprocessed NetCDF data (train/val sets)\n",
    "2. Instantiate the DeepESD model\n",
    "3. Train it using MSE loss\n",
    "4. Save the trained model for inference and XAI\n"
   ],
   "id": "fd4ce02586f56056"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:03:05.108549Z",
     "start_time": "2025-05-11T19:03:05.103679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "from xbatcher import BatchGenerator\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {DEVICE}\")"
   ],
   "id": "2897211511346be5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:03:05.186777Z",
     "start_time": "2025-05-11T19:03:05.184374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepESD(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: tuple,\n",
    "        output_shape: tuple,\n",
    "        input_channels: int,\n",
    "        output_channels: int,\n",
    "    ):\n",
    "        super(DeepESD, self).__init__()\n",
    "        self.output_shape = output_shape  # Store as attribute\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, 50, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(50, 25, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(25, output_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        in_features = input_shape[0] * input_shape[1] * output_channels\n",
    "        out_features = output_shape[0] * output_shape[1] * output_channels\n",
    "        self.out = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.out(x)\n",
    "        return x.view(\n",
    "            -1, self.output_channels, self.output_shape[0], self.output_shape[1]\n",
    "        )"
   ],
   "id": "a253af140ba10a1d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:03:05.251484Z",
     "start_time": "2025-05-11T19:03:05.247363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2D => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features=[64, 128, 256, 512]):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Downsampling path\n",
    "        for feature in features:\n",
    "            self.encoder.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
    "\n",
    "        # Upsampling path\n",
    "        self.up_transpose = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for feature in reversed(features):\n",
    "            self.up_transpose.append(\n",
    "                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.decoder.append(DoubleConv(feature * 2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.encoder:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(len(self.up_transpose)):\n",
    "            x = self.up_transpose[idx](x)\n",
    "            skip_connection = skip_connections[idx]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = F.interpolate(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            x = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.decoder[idx](x)\n",
    "\n",
    "        return self.final_conv(x)"
   ],
   "id": "288c3deb400c9122",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:03:05.321815Z",
     "start_time": "2025-05-11T19:03:05.318358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_netcdf_pair(x_path, y_path, batch_size=16, variable_name=\"t2m\", shuffle=True):\n",
    "    ds_x = xr.open_dataset(x_path)\n",
    "    ds_y = xr.open_dataset(y_path)\n",
    "    ds_x = ds_x.transpose(\"time\", \"lat\", \"lon\")\n",
    "    ds_y = ds_y.transpose(\"time\", \"lat\", \"lon\")\n",
    "\n",
    "    # Ensure they share the same time steps (intersection only)\n",
    "    common_times = np.intersect1d(ds_x[\"time\"].values, ds_y[\"time\"].values)\n",
    "    ds_x = ds_x.sel(time=common_times)\n",
    "    ds_y = ds_y.sel(time=common_times)\n",
    "\n",
    "    if len(common_times) == 0:\n",
    "        raise ValueError(f\"No overlapping timestamps between {x_path} and {y_path}\")\n",
    "\n",
    "    x_gen = BatchGenerator(\n",
    "        ds_x[[variable_name]],\n",
    "        input_dims={\n",
    "            \"time\": batch_size,\n",
    "            \"lat\": len(ds_x.lat.values),\n",
    "            \"lon\": len(ds_x.lon.values),\n",
    "        },\n",
    "        preload_batch=False,\n",
    "    )\n",
    "\n",
    "    y_gen = BatchGenerator(\n",
    "        ds_y[[variable_name]],\n",
    "        input_dims={\n",
    "            \"time\": batch_size,\n",
    "            \"lat\": len(ds_y.lat.values),\n",
    "            \"lon\": len(ds_y.lon.values),\n",
    "        },\n",
    "        preload_batch=False,\n",
    "    )\n",
    "\n",
    "    def batch_to_tensor(x_batch, y_batch, variable_name=\"t2m\"):\n",
    "        x_arr = x_batch[variable_name].values\n",
    "        y_arr = y_batch[variable_name].values\n",
    "\n",
    "        # Ensure 3D shape before adding channel dim\n",
    "        if x_arr.ndim == 2:\n",
    "            x_arr = x_arr[None, :, :]  # Add time dim\n",
    "        if y_arr.ndim == 2:\n",
    "            y_arr = y_arr[None, :, :]\n",
    "\n",
    "        x = torch.tensor(x_arr[:, None, :, :], dtype=torch.float32)\n",
    "        y = torch.tensor(y_arr[:, None, :, :], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "    data = [batch_to_tensor(xb, yb) for xb, yb in zip(x_gen, y_gen)]\n",
    "\n",
    "    return DataLoader(data, batch_size=None, shuffle=shuffle)"
   ],
   "id": "5b907ec5a29762f0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:35:03.106101Z",
     "start_time": "2025-05-11T20:30:13.945082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# File paths\n",
    "train_era5 = \"../data/train_era5.nc\"\n",
    "train_cerra = \"../data/train_cerra.nc\"\n",
    "val_era5 = \"../data/val_era5.nc\"\n",
    "val_cerra = \"../data/val_cerra.nc\"\n",
    "model_path = \"../models/deepesd_trained.pt\"\n",
    "metrics_path = \"../models/metrics.csv\"\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "lr = 1e-4\n",
    "batch_size = 16\n",
    "\n",
    "# Load data\n",
    "logging.info(\"Creating dataloaders...\")\n",
    "logging.info(\"Creating dataloader for training...\")\n",
    "train_dataloader = load_netcdf_pair(\n",
    "    train_era5, train_cerra, variable_name=\"t2m\", batch_size=batch_size, shuffle=True\n",
    ")\n",
    "logging.info(\"Creating dataloader for validation...\")\n",
    "val_dataloader = load_netcdf_pair(\n",
    "    val_era5, val_cerra, variable_name=\"t2m\", batch_size=batch_size, shuffle=False\n",
    ")\n",
    "logging.info(\"Dataloaders created.\")\n",
    "\n",
    "# Model setup\n",
    "input_shape = train_dataloader.dataset[0][0].shape[-2:]\n",
    "output_shape = train_dataloader.dataset[0][1].shape[-2:]\n",
    "\n",
    "model = DeepESD(\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    input_channels=1,\n",
    "    output_channels=1,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "logging.info(\n",
    "    f\"Model initialized with input shape {input_shape}, output shape {output_shape}\"\n",
    ")\n",
    "\n",
    "# Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "patience = 10  # Early stopping patience\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "# Early stopping vars\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "# Logging losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "logging.info(\"Starting training loop...\")\n",
    "for epoch in range(epochs):\n",
    "    if early_stop:\n",
    "        break\n",
    "\n",
    "    logging.info(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    train_samples = tqdm(\n",
    "        train_dataloader, desc=f\"[Epoch {epoch + 1}] Training\", leave=False\n",
    "    )\n",
    "\n",
    "    for train_predictor, train_target in train_samples:\n",
    "        train_predictor, train_target = train_predictor.to(DEVICE), train_target.to(\n",
    "            DEVICE\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(train_predictor)\n",
    "        loss = criterion(prediction, train_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "        train_samples.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    val_samples = tqdm(\n",
    "        val_dataloader, desc=f\"[Epoch {epoch + 1}] Validation\", leave=False\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_predictor, val_target in val_samples:\n",
    "            val_predictor, val_target = val_predictor.to(DEVICE), val_target.to(DEVICE)\n",
    "            pred = model(val_predictor)\n",
    "            loss = criterion(pred, val_target)\n",
    "            epoch_val_loss += loss.item()\n",
    "            val_samples.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Log losses\n",
    "    logging.info(\n",
    "        f\"Epoch {epoch+1} â€” Train Loss: {avg_train_loss:.6f} â€” Val Loss: {avg_val_loss:.6f} â€” LR: {scheduler.get_last_lr()[0]:.6e}\"\n",
    "    )\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss <= best_val_loss * 0.95:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        logging.info(f\"âœ… Model improved and saved to {model_path}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        logging.info(f\"No improvement for {epochs_no_improve} epoch(s)\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        logging.info(\n",
    "            f\"ðŸ›‘ Early stopping triggered after {patience} epochs without improvement.\"\n",
    "        )\n",
    "        early_stop = True\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"epoch\": list(range(1, len(train_losses) + 1)),\n",
    "        \"train_loss\": train_losses,\n",
    "        \"val_loss\": val_losses,\n",
    "    }\n",
    ")\n",
    "os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "logging.info(f\"ðŸ“ˆ Metrics saved to {metrics_path}\")"
   ],
   "id": "ad71a76b80a5f3bf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating dataloaders...\n",
      "INFO:root:Creating dataloader for training...\n",
      "INFO:root:Creating dataloader for validation...\n",
      "INFO:root:Dataloaders created.\n",
      "INFO:root:Model initialized with input spatial dim torch.Size([63, 65]), target dim 43452\n",
      "INFO:root:Starting training loop...\n",
      "INFO:root:Epoch 1/500\n",
      "INFO:root:Epoch 1 â€” Train Loss: 181.122335 â€” Val Loss: 140.233745 â€” LR: 9.999901e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 2/500\n",
      "INFO:root:Epoch 2 â€” Train Loss: 77.742674 â€” Val Loss: 26.018454 â€” LR: 9.999605e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 3/500\n",
      "INFO:root:Epoch 3 â€” Train Loss: 18.045032 â€” Val Loss: 15.617580 â€” LR: 9.999112e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 4/500\n",
      "INFO:root:Epoch 4 â€” Train Loss: 14.909537 â€” Val Loss: 14.382968 â€” LR: 9.998421e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 5/500\n",
      "INFO:root:Epoch 5 â€” Train Loss: 13.797331 â€” Val Loss: 13.392713 â€” LR: 9.997533e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 6/500\n",
      "INFO:root:Epoch 6 â€” Train Loss: 12.923465 â€” Val Loss: 12.568351 â€” LR: 9.996447e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 7/500\n",
      "INFO:root:Epoch 7 â€” Train Loss: 12.093018 â€” Val Loss: 11.775879 â€” LR: 9.995165e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 8/500\n",
      "INFO:root:Epoch 8 â€” Train Loss: 11.384836 â€” Val Loss: 11.088603 â€” LR: 9.993685e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 9/500\n",
      "INFO:root:Epoch 9 â€” Train Loss: 10.721175 â€” Val Loss: 10.486741 â€” LR: 9.992008e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 10/500\n",
      "INFO:root:Epoch 10 â€” Train Loss: 10.125190 â€” Val Loss: 9.928048 â€” LR: 9.990134e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 11/500\n",
      "INFO:root:Epoch 11 â€” Train Loss: 9.590235 â€” Val Loss: 9.397930 â€” LR: 9.988063e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 12/500\n",
      "INFO:root:Epoch 12 â€” Train Loss: 9.097210 â€” Val Loss: 8.966046 â€” LR: 9.985795e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 13/500\n",
      "INFO:root:Epoch 13 â€” Train Loss: 8.649713 â€” Val Loss: 8.537684 â€” LR: 9.983330e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 14/500\n",
      "INFO:root:Epoch 14 â€” Train Loss: 8.255729 â€” Val Loss: 8.185303 â€” LR: 9.980668e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 15/500\n",
      "INFO:root:Epoch 15 â€” Train Loss: 7.910971 â€” Val Loss: 7.841962 â€” LR: 9.977810e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 16/500\n",
      "INFO:root:Epoch 16 â€” Train Loss: 7.602961 â€” Val Loss: 7.577866 â€” LR: 9.974755e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 17/500\n",
      "INFO:root:Epoch 17 â€” Train Loss: 7.334800 â€” Val Loss: 7.325715 â€” LR: 9.971504e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 18/500\n",
      "INFO:root:Epoch 18 â€” Train Loss: 7.106513 â€” Val Loss: 7.168303 â€” LR: 9.968057e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 19/500\n",
      "INFO:root:Epoch 19 â€” Train Loss: 6.903050 â€” Val Loss: 6.948182 â€” LR: 9.964413e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 20/500\n",
      "INFO:root:Epoch 20 â€” Train Loss: 6.724459 â€” Val Loss: 6.806311 â€” LR: 9.960574e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 21/500\n",
      "INFO:root:Epoch 21 â€” Train Loss: 6.574689 â€” Val Loss: 6.635749 â€” LR: 9.956538e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 22/500\n",
      "INFO:root:Epoch 22 â€” Train Loss: 6.425275 â€” Val Loss: 6.479241 â€” LR: 9.952307e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 23/500\n",
      "INFO:root:Epoch 23 â€” Train Loss: 6.282866 â€” Val Loss: 6.373670 â€” LR: 9.947881e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 24/500\n",
      "INFO:root:Epoch 24 â€” Train Loss: 6.169200 â€” Val Loss: 6.249070 â€” LR: 9.943259e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 25/500\n",
      "INFO:root:Epoch 25 â€” Train Loss: 6.035381 â€” Val Loss: 6.105491 â€” LR: 9.938442e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 26/500\n",
      "INFO:root:Epoch 26 â€” Train Loss: 5.921302 â€” Val Loss: 5.992688 â€” LR: 9.933430e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 27/500\n",
      "INFO:root:Epoch 27 â€” Train Loss: 5.813376 â€” Val Loss: 5.882961 â€” LR: 9.928223e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 28/500\n",
      "INFO:root:Epoch 28 â€” Train Loss: 5.705030 â€” Val Loss: 5.776684 â€” LR: 9.922822e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 29/500\n",
      "INFO:root:Epoch 29 â€” Train Loss: 5.598167 â€” Val Loss: 5.669415 â€” LR: 9.917226e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 30/500\n",
      "INFO:root:Epoch 30 â€” Train Loss: 5.498087 â€” Val Loss: 5.595055 â€” LR: 9.911436e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 31/500\n",
      "INFO:root:Epoch 31 â€” Train Loss: 5.423459 â€” Val Loss: 5.483781 â€” LR: 9.905453e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 32/500\n",
      "INFO:root:Epoch 32 â€” Train Loss: 5.324895 â€” Val Loss: 5.402832 â€” LR: 9.899275e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 33/500\n",
      "INFO:root:Epoch 33 â€” Train Loss: 5.243069 â€” Val Loss: 5.315425 â€” LR: 9.892905e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 34/500\n",
      "INFO:root:Epoch 34 â€” Train Loss: 5.161867 â€” Val Loss: 5.238416 â€” LR: 9.886341e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 35/500\n",
      "INFO:root:Epoch 35 â€” Train Loss: 5.088606 â€” Val Loss: 5.168100 â€” LR: 9.879584e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 36/500\n",
      "INFO:root:Epoch 36 â€” Train Loss: 5.011811 â€” Val Loss: 5.088356 â€” LR: 9.872634e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 37/500\n",
      "INFO:root:Epoch 37 â€” Train Loss: 4.944419 â€” Val Loss: 5.008187 â€” LR: 9.865493e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 38/500\n",
      "INFO:root:Epoch 38 â€” Train Loss: 4.877604 â€” Val Loss: 4.940108 â€” LR: 9.858159e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 39/500\n",
      "INFO:root:Epoch 39 â€” Train Loss: 4.808722 â€” Val Loss: 4.873876 â€” LR: 9.850633e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 40/500\n",
      "INFO:root:Epoch 40 â€” Train Loss: 4.745157 â€” Val Loss: 4.812145 â€” LR: 9.842916e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 41/500\n",
      "INFO:root:Epoch 41 â€” Train Loss: 4.676466 â€” Val Loss: 4.731408 â€” LR: 9.835007e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 42/500\n",
      "INFO:root:Epoch 42 â€” Train Loss: 4.609673 â€” Val Loss: 4.667628 â€” LR: 9.826908e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 43/500\n",
      "INFO:root:Epoch 43 â€” Train Loss: 4.540036 â€” Val Loss: 4.595904 â€” LR: 9.818618e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 44/500\n",
      "INFO:root:Epoch 44 â€” Train Loss: 4.468582 â€” Val Loss: 4.533695 â€” LR: 9.810138e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 45/500\n",
      "INFO:root:Epoch 45 â€” Train Loss: 4.394185 â€” Val Loss: 4.459970 â€” LR: 9.801468e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 46/500\n",
      "INFO:root:Epoch 46 â€” Train Loss: 4.341090 â€” Val Loss: 4.376286 â€” LR: 9.792609e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 47/500\n",
      "INFO:root:Epoch 47 â€” Train Loss: 4.261248 â€” Val Loss: 4.306684 â€” LR: 9.783560e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 48/500\n",
      "INFO:root:Epoch 48 â€” Train Loss: 4.183276 â€” Val Loss: 4.226062 â€” LR: 9.774323e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 49/500\n",
      "INFO:root:Epoch 49 â€” Train Loss: 4.111709 â€” Val Loss: 4.153359 â€” LR: 9.764897e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 50/500\n",
      "INFO:root:Epoch 50 â€” Train Loss: 4.036415 â€” Val Loss: 4.102898 â€” LR: 9.755283e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 51/500\n",
      "INFO:root:Epoch 51 â€” Train Loss: 3.962908 â€” Val Loss: 4.005172 â€” LR: 9.745481e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 52/500\n",
      "INFO:root:Epoch 52 â€” Train Loss: 3.894761 â€” Val Loss: 3.937624 â€” LR: 9.735492e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 53/500\n",
      "INFO:root:Epoch 53 â€” Train Loss: 3.828159 â€” Val Loss: 3.862744 â€” LR: 9.725315e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 54/500\n",
      "INFO:root:Epoch 54 â€” Train Loss: 3.756490 â€” Val Loss: 3.831803 â€” LR: 9.714953e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 55/500\n",
      "INFO:root:Epoch 55 â€” Train Loss: 3.703993 â€” Val Loss: 3.733292 â€” LR: 9.704404e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 56/500\n",
      "INFO:root:Epoch 56 â€” Train Loss: 3.638188 â€” Val Loss: 3.677208 â€” LR: 9.693669e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 57/500\n",
      "INFO:root:Epoch 57 â€” Train Loss: 3.593916 â€” Val Loss: 3.622512 â€” LR: 9.682749e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 58/500\n",
      "INFO:root:Epoch 58 â€” Train Loss: 3.539163 â€” Val Loss: 3.592994 â€” LR: 9.671645e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 59/500\n",
      "INFO:root:Epoch 59 â€” Train Loss: 3.489001 â€” Val Loss: 3.538026 â€” LR: 9.660356e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 60/500\n",
      "INFO:root:Epoch 60 â€” Train Loss: 3.450517 â€” Val Loss: 3.482612 â€” LR: 9.648882e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 61/500\n",
      "INFO:root:Epoch 61 â€” Train Loss: 3.404899 â€” Val Loss: 3.437543 â€” LR: 9.637226e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 62/500\n",
      "INFO:root:Epoch 62 â€” Train Loss: 3.377142 â€” Val Loss: 3.398645 â€” LR: 9.625386e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 63/500\n",
      "INFO:root:Epoch 63 â€” Train Loss: 3.337282 â€” Val Loss: 3.378010 â€” LR: 9.613364e-06\n",
      "INFO:root:No improvement for 4 epoch(s)\n",
      "INFO:root:Epoch 64/500\n",
      "INFO:root:Epoch 64 â€” Train Loss: 3.310724 â€” Val Loss: 3.318336 â€” LR: 9.601159e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 65/500\n",
      "INFO:root:Epoch 65 â€” Train Loss: 3.292617 â€” Val Loss: 3.309093 â€” LR: 9.588773e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 66/500\n",
      "INFO:root:Epoch 66 â€” Train Loss: 3.250493 â€” Val Loss: 3.290468 â€” LR: 9.576206e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 67/500\n",
      "INFO:root:Epoch 67 â€” Train Loss: 3.230868 â€” Val Loss: 3.325202 â€” LR: 9.563458e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 68/500\n",
      "INFO:root:Epoch 68 â€” Train Loss: 3.216801 â€” Val Loss: 3.223753 â€” LR: 9.550530e-06\n",
      "INFO:root:No improvement for 4 epoch(s)\n",
      "INFO:root:Epoch 69/500\n",
      "INFO:root:Epoch 69 â€” Train Loss: 3.189154 â€” Val Loss: 3.195922 â€” LR: 9.537422e-06\n",
      "INFO:root:No improvement for 5 epoch(s)\n",
      "INFO:root:Epoch 70/500\n",
      "INFO:root:Epoch 70 â€” Train Loss: 3.165974 â€” Val Loss: 3.165808 â€” LR: 9.524135e-06\n",
      "INFO:root:No improvement for 6 epoch(s)\n",
      "INFO:root:Epoch 71/500\n",
      "INFO:root:Epoch 71 â€” Train Loss: 3.140928 â€” Val Loss: 3.148172 â€” LR: 9.510670e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 72/500\n",
      "INFO:root:Epoch 72 â€” Train Loss: 3.121657 â€” Val Loss: 3.124348 â€” LR: 9.497026e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 73/500\n",
      "INFO:root:Epoch 73 â€” Train Loss: 3.107430 â€” Val Loss: 3.121197 â€” LR: 9.483205e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 74/500\n",
      "INFO:root:Epoch 74 â€” Train Loss: 3.103487 â€” Val Loss: 3.125009 â€” LR: 9.469207e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 75/500\n",
      "INFO:root:Epoch 75 â€” Train Loss: 3.063877 â€” Val Loss: 3.082113 â€” LR: 9.455033e-06\n",
      "INFO:root:No improvement for 4 epoch(s)\n",
      "INFO:root:Epoch 76/500\n",
      "INFO:root:Epoch 76 â€” Train Loss: 3.054763 â€” Val Loss: 3.071324 â€” LR: 9.440682e-06\n",
      "INFO:root:No improvement for 5 epoch(s)\n",
      "INFO:root:Epoch 77/500\n",
      "INFO:root:Epoch 77 â€” Train Loss: 3.027408 â€” Val Loss: 3.052508 â€” LR: 9.426157e-06\n",
      "INFO:root:No improvement for 6 epoch(s)\n",
      "INFO:root:Epoch 78/500\n",
      "INFO:root:Epoch 78 â€” Train Loss: 3.014781 â€” Val Loss: 3.024174 â€” LR: 9.411456e-06\n",
      "INFO:root:No improvement for 7 epoch(s)\n",
      "INFO:root:Epoch 79/500\n",
      "INFO:root:Epoch 79 â€” Train Loss: 2.996029 â€” Val Loss: 3.050091 â€” LR: 9.396582e-06\n",
      "INFO:root:No improvement for 8 epoch(s)\n",
      "INFO:root:Epoch 80/500\n",
      "INFO:root:Epoch 80 â€” Train Loss: 2.980347 â€” Val Loss: 2.986455 â€” LR: 9.381533e-06\n",
      "INFO:root:âœ… Model improved and saved to ../models/deepesd_trained.pt\n",
      "INFO:root:Epoch 81/500\n",
      "INFO:root:Epoch 81 â€” Train Loss: 2.961814 â€” Val Loss: 2.985248 â€” LR: 9.366312e-06\n",
      "INFO:root:No improvement for 1 epoch(s)\n",
      "INFO:root:Epoch 82/500\n",
      "INFO:root:Epoch 82 â€” Train Loss: 2.946675 â€” Val Loss: 2.979482 â€” LR: 9.350919e-06\n",
      "INFO:root:No improvement for 2 epoch(s)\n",
      "INFO:root:Epoch 83/500\n",
      "INFO:root:Epoch 83 â€” Train Loss: 2.939673 â€” Val Loss: 2.941853 â€” LR: 9.335354e-06\n",
      "INFO:root:No improvement for 3 epoch(s)\n",
      "INFO:root:Epoch 84/500\n",
      "INFO:root:Epoch 84 â€” Train Loss: 2.928941 â€” Val Loss: 2.947457 â€” LR: 9.319617e-06\n",
      "INFO:root:No improvement for 4 epoch(s)\n",
      "INFO:root:Epoch 85/500\n",
      "INFO:root:Epoch 85 â€” Train Loss: 2.922966 â€” Val Loss: 2.930033 â€” LR: 9.303710e-06\n",
      "INFO:root:No improvement for 5 epoch(s)\n",
      "INFO:root:Epoch 86/500\n",
      "INFO:root:Epoch 86 â€” Train Loss: 2.892777 â€” Val Loss: 2.922862 â€” LR: 9.287633e-06\n",
      "INFO:root:No improvement for 6 epoch(s)\n",
      "INFO:root:Epoch 87/500\n",
      "INFO:root:Epoch 87 â€” Train Loss: 2.888421 â€” Val Loss: 2.909086 â€” LR: 9.271387e-06\n",
      "INFO:root:No improvement for 7 epoch(s)\n",
      "INFO:root:Epoch 88/500\n",
      "INFO:root:Epoch 88 â€” Train Loss: 2.875122 â€” Val Loss: 2.893016 â€” LR: 9.254972e-06\n",
      "INFO:root:No improvement for 8 epoch(s)\n",
      "INFO:root:Epoch 89/500\n",
      "INFO:root:Epoch 89 â€” Train Loss: 2.859452 â€” Val Loss: 2.861452 â€” LR: 9.238390e-06\n",
      "INFO:root:No improvement for 9 epoch(s)\n",
      "INFO:root:Epoch 90/500\n",
      "INFO:root:Epoch 90 â€” Train Loss: 2.868170 â€” Val Loss: 2.884447 â€” LR: 9.221640e-06\n",
      "INFO:root:No improvement for 10 epoch(s)\n",
      "INFO:root:ðŸ›‘ Early stopping triggered after 10 epochs without improvement.\n",
      "INFO:root:ðŸ“ˆ Metrics saved to ../models/metrics.csv\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "17931ce06ce91eee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
