{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explainability for Deep Learning Climate Downscaling\n",
    "\n",
    "This notebook demonstrates how to apply **Explainable AI (XAI)** techniques to deep learning models trained for spatial downscaling from ERA5 to CERRA temperature data over South-East Europe (SEE). We will use the [Quantus](https://github.com/understandable-machine-intelligence-lab/quantus) library to apply and evaluate saliency-based XAI methods.\n",
    "\n",
    "## ðŸ” Goals\n",
    "- Load trained models and test data\n",
    "- Generate saliency maps using Quantus\n",
    "- Compare explanation patterns between DeepESD and U-Net\n",
    "- Visualize attribution over the spatial domain"
   ],
   "id": "4382c61eb0f5fed8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:49:47.419854Z",
     "start_time": "2025-05-13T10:49:47.406128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(\"ðŸ” Starting imports...\")\n",
    "\n",
    "try:\n",
    "    import os\n",
    "\n",
    "    logging.info(\"âœ… Imported os\")\n",
    "\n",
    "    import torch\n",
    "\n",
    "    logging.info(\"âœ… Imported torch\")\n",
    "\n",
    "    import xarray as xr\n",
    "\n",
    "    logging.info(\"âœ… Imported xarray\")\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    logging.info(\"âœ… Imported numpy\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    logging.info(\"âœ… Imported matplotlib\")\n",
    "\n",
    "    import quantus\n",
    "\n",
    "    logging.info(\"âœ… Imported quantus\")\n",
    "\n",
    "    from xbatcher import BatchGenerator\n",
    "\n",
    "    logging.info(\"âœ… Imported xbatcher\")\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    logging.info(\"âœ… Imported torch.utils.data\")\n",
    "\n",
    "    from IPython.display import display, Image\n",
    "\n",
    "    logging.info(\"âœ… Imported IPython.display\")\n",
    "\n",
    "    import cartopy.crs as ccrs\n",
    "\n",
    "    logging.info(\"âœ… Imported cartopy.crs\")\n",
    "\n",
    "    import cartopy.feature as cfeature\n",
    "\n",
    "    logging.info(\"âœ… Imported cartopy.feature\")\n",
    "\n",
    "    import warnings\n",
    "\n",
    "    logging.info(\"âœ… Imported warnings\")\n",
    "\n",
    "    from source.model_deepesd import DeepESD\n",
    "    from source.model_unet import UNet\n",
    "\n",
    "    logging.info(\"âœ… Imported local models\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"âŒ Import failed: {e}\")\n",
    "    raise e\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"ðŸ“¦ Using device: {DEVICE}\")"
   ],
   "id": "7eebc11dc6d13b56",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ðŸ” Starting imports...\n",
      "INFO:root:âœ… Imported os\n",
      "INFO:root:âœ… Imported torch\n",
      "INFO:root:âœ… Imported xarray\n",
      "INFO:root:âœ… Imported numpy\n",
      "INFO:root:âœ… Imported matplotlib\n",
      "INFO:root:âœ… Imported quantus\n",
      "INFO:root:âœ… Imported xbatcher\n",
      "INFO:root:âœ… Imported torch.utils.data\n",
      "INFO:root:âœ… Imported IPython.display\n",
      "INFO:root:âœ… Imported cartopy.crs\n",
      "INFO:root:âœ… Imported cartopy.feature\n",
      "INFO:root:âœ… Imported warnings\n",
      "INFO:root:âœ… Imported local models\n",
      "INFO:root:ðŸ“¦ Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“¥ Load Test Data and Models\n",
    "\n",
    "In this section, we load the preprocessed test data from ERA5 (inputs) and CERRA (targets), and load the trained models stored on disk."
   ],
   "id": "24a8ca7d925d8a23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:50:33.965428Z",
     "start_time": "2025-05-13T10:49:47.479294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from source.generate_dataloader import load_netcdf_pair\n",
    "logging.info(\"Generating test dataloader...\")\n",
    "\n",
    "# Paths\n",
    "test_era5 = \"../data/test_era5.nc\"\n",
    "test_cerra = \"../data/test_cerra.nc\"\n",
    "\n",
    "test_dataloader = load_netcdf_pair(test_era5, test_cerra)\n",
    "input_sample, target_sample = test_dataloader.dataset[0]\n",
    "logging.info(\"Test dataloader created successfully\")\n",
    "logging.info(f\"Loaded one test sample with shape: {input_sample.shape}\")"
   ],
   "id": "c61dcd3fdf291534",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ðŸ“¥ Loading test data from NetCDF...\n",
      "INFO:root:âœ… Loaded one test sample with shape: torch.Size([16, 1, 63, 65])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ”§ Load Trained DeepESD and U-Net Models\n",
    "\n",
    "Now we initialize both architectures and load their pre-trained weights. These models were trained for temperature downscaling in the WS4 training notebook.\n"
   ],
   "id": "fb0121ee2588a7b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:50:35.580415Z",
     "start_time": "2025-05-13T10:50:34.026522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths to trained models\n",
    "model_path_deepesd = \"../models/model_deepesd.pt\"\n",
    "model_path_unet = \"../models/model_unet.pt\"\n",
    "\n",
    "# Infer shapes from test data\n",
    "input_shape = input_sample.shape[-2:]\n",
    "output_shape = target_sample.shape[-2:]\n",
    "\n",
    "logging.info(\"ðŸ”§ Loading DeepESD model...\")\n",
    "deepesd = DeepESD(input_shape, output_shape, 1, 1)\n",
    "deepesd.load_state_dict(torch.load(model_path_deepesd, map_location=DEVICE))\n",
    "deepesd.to(DEVICE).eval()\n",
    "logging.info(\"âœ… DeepESD loaded.\")\n",
    "\n",
    "logging.info(\"ðŸ”§ Loading U-Net model...\")\n",
    "unet = UNet(input_shape, output_shape, 1, 1)\n",
    "unet.load_state_dict(torch.load(model_path_unet, map_location=DEVICE))\n",
    "unet.to(DEVICE).eval()\n",
    "logging.info(\"âœ… U-Net loaded.\")\n"
   ],
   "id": "4b56295cf3d986b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ðŸ”§ Loading DeepESD model...\n",
      "INFO:root:âœ… DeepESD loaded.\n",
      "INFO:root:ðŸ”§ Loading U-Net model...\n",
      "INFO:root:âœ… U-Net loaded.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load or Generate Explanations\n",
    "\n",
    "In order to evaluate explainability techniques with Quantus, we need a batch of **inputs**, **targets**, and corresponding **attributions (explanations)**.\n",
    "\n",
    "There are two options:\n",
    "\n",
    "- ðŸ” **Option A: Use pre-computed attributions**, e.g., from Captum\n",
    "- ðŸ§  **Option B: Use a callable explanation function**, such as `quantus.explain()` or a custom function\n",
    "\n",
    "In this example, we use **Captum** to compute both **Saliency** and **Integrated Gradients** attributions for a single input batch from our test dataset.\n"
   ],
   "id": "4a3760ed6916b563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:50:36.206596Z",
     "start_time": "2025-05-13T10:50:35.599281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from captum.attr import Saliency, IntegratedGradients\n",
    "\n",
    "logging.info(\"ðŸ“¥ Sampling input batch for explanation...\")\n",
    "x_batch, y_batch = next(iter(test_dataloader))\n",
    "x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "logging.info(\"ðŸ§  Generating Saliency and Integrated Gradients attributions with Captum...\")\n",
    "saliency_attr = Saliency(deepesd).attribute(inputs=x_batch, target=None, abs=True).sum(dim=1).detach().cpu().numpy()\n",
    "intgrad_attr = IntegratedGradients(deepesd).attribute(\n",
    "    inputs=x_batch, target=None, baselines=torch.zeros_like(x_batch)\n",
    ").sum(dim=1).detach().cpu().numpy()\n",
    "\n",
    "# Convert input and target to numpy\n",
    "x_batch_np = x_batch.detach().cpu().numpy()\n",
    "y_batch_np = y_batch.detach().cpu().numpy()\n",
    "\n",
    "# Quick assertion to confirm shapes and types\n",
    "assert all(isinstance(arr, np.ndarray) for arr in [x_batch_np, y_batch_np, saliency_attr, intgrad_attr])\n",
    "logging.info(f\"âœ… Shapes â€” X: {x_batch_np.shape}, Y: {y_batch_np.shape}, Saliency: {saliency_attr.shape}, IG: {intgrad_attr.shape}\")\n"
   ],
   "id": "6d9eb93adea16a14",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ðŸ“¥ Sampling input batch for explanation...\n",
      "INFO:root:ðŸ§  Generating Saliency and Integrated Gradients attributions with Captum...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Target not provided when necessary, cannot take gradient with respect to multiple outputs.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n\u001B[32m      7\u001B[39m logging.info(\u001B[33m\"\u001B[39m\u001B[33mðŸ§  Generating Saliency and Integrated Gradients attributions with Captum...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m saliency_attr = \u001B[43mSaliency\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeepesd\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mattribute\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mabs\u001B[39;49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m.sum(dim=\u001B[32m1\u001B[39m).detach().cpu().numpy()\n\u001B[32m      9\u001B[39m intgrad_attr = IntegratedGradients(deepesd).attribute(\n\u001B[32m     10\u001B[39m     inputs=x_batch, target=\u001B[38;5;28;01mNone\u001B[39;00m, baselines=torch.zeros_like(x_batch)\n\u001B[32m     11\u001B[39m ).sum(dim=\u001B[32m1\u001B[39m).detach().cpu().numpy()\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Convert input and target to numpy\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/clearclimate-ws4/lib/python3.11/site-packages/captum/log/dummy_log.py:39\u001B[39m, in \u001B[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[32m     36\u001B[39m \u001B[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001B[39;00m\n\u001B[32m     37\u001B[39m \u001B[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001B[39;00m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper\u001B[39m(*args: Any, **kwargs: Any):\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/clearclimate-ws4/lib/python3.11/site-packages/captum/attr/_core/saliency.py:133\u001B[39m, in \u001B[36mSaliency.attribute\u001B[39m\u001B[34m(self, inputs, target, abs, additional_forward_args)\u001B[39m\n\u001B[32m    129\u001B[39m gradient_mask = apply_gradient_requirements(inputs_tuple)\n\u001B[32m    131\u001B[39m \u001B[38;5;66;03m# No need to format additional_forward_args here.\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[38;5;66;03m# They are being formated in the `_run_forward` function in `common.py`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m gradients = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgradient_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madditional_forward_args\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mabs\u001B[39m:\n\u001B[32m    137\u001B[39m     attributions = \u001B[38;5;28mtuple\u001B[39m(torch.abs(gradient) \u001B[38;5;28;01mfor\u001B[39;00m gradient \u001B[38;5;129;01min\u001B[39;00m gradients)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/clearclimate-ws4/lib/python3.11/site-packages/captum/_utils/gradient.py:133\u001B[39m, in \u001B[36mcompute_gradients\u001B[39m\u001B[34m(forward_fn, inputs, target_ind, additional_forward_args)\u001B[39m\n\u001B[32m    129\u001B[39m \u001B[38;5;66;03m# _run_forward may return future of Tensor,\u001B[39;00m\n\u001B[32m    130\u001B[39m \u001B[38;5;66;03m# but we don't support it here now\u001B[39;00m\n\u001B[32m    131\u001B[39m \u001B[38;5;66;03m# And it will fail before here.\u001B[39;00m\n\u001B[32m    132\u001B[39m outputs = cast(Tensor, outputs)\n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m outputs[\u001B[32m0\u001B[39m].numel() == \u001B[32m1\u001B[39m, (\n\u001B[32m    134\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mTarget not provided when necessary, cannot\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    135\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m take gradient with respect to multiple outputs.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    136\u001B[39m )\n\u001B[32m    137\u001B[39m \u001B[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001B[39;00m\n\u001B[32m    138\u001B[39m \u001B[38;5;66;03m# contains batch_size * #steps elements\u001B[39;00m\n\u001B[32m    139\u001B[39m grads = torch.autograd.grad(torch.unbind(outputs), inputs)\n",
      "\u001B[31mAssertionError\u001B[39m: Target not provided when necessary, cannot take gradient with respect to multiple outputs."
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e9f0d8441ec844dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
