{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explainability for Deep Learning Climate Downscaling\n",
    "\n",
    "This notebook demonstrates how to apply **Explainable AI (XAI)** techniques to deep learning models trained for spatial downscaling from ERA5 to CERRA temperature data over South-East Europe (SEE). We will use the [Quantus](https://github.com/understandable-machine-intelligence-lab/quantus) library to apply and evaluate saliency-based XAI methods.\n",
    "\n",
    "## 🔍 Goals\n",
    "- Load trained models and test data\n",
    "- Generate saliency maps using Quantus\n",
    "- Compare explanation patterns between DeepESD and U-Net\n",
    "- Visualize attribution over the spatial domain"
   ],
   "id": "4382c61eb0f5fed8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(\"🔁 Starting imports...\")\n",
    "\n",
    "try:\n",
    "    import os\n",
    "\n",
    "    logging.info(\"✅ Imported os\")\n",
    "\n",
    "    import torch\n",
    "\n",
    "    logging.info(\"✅ Imported torch\")\n",
    "\n",
    "    import xarray as xr\n",
    "\n",
    "    logging.info(\"✅ Imported xarray\")\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    logging.info(\"✅ Imported numpy\")\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    logging.info(\"✅ Imported pandas\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    logging.info(\"✅ Imported matplotlib\")\n",
    "\n",
    "    import quantus\n",
    "\n",
    "    logging.info(\"✅ Imported quantus\")\n",
    "\n",
    "    from xbatcher import BatchGenerator\n",
    "\n",
    "    logging.info(\"✅ Imported xbatcher\")\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    logging.info(\"✅ Imported torch.utils.data\")\n",
    "\n",
    "    from IPython.display import display, Image\n",
    "\n",
    "    logging.info(\"✅ Imported IPython.display\")\n",
    "\n",
    "    import cartopy.crs as ccrs\n",
    "\n",
    "    logging.info(\"✅ Imported cartopy.crs\")\n",
    "\n",
    "    import cartopy.feature as cfeature\n",
    "\n",
    "    logging.info(\"✅ Imported cartopy.feature\")\n",
    "\n",
    "    import warnings\n",
    "\n",
    "    logging.info(\"✅ Imported warnings\")\n",
    "\n",
    "    from source.model_deepesd import DeepESD\n",
    "    from source.model_unet import UNet\n",
    "\n",
    "    logging.info(\"✅ Imported local models\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"❌ Import failed: {e}\")\n",
    "    raise e\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"📦 Using device: {DEVICE}\")"
   ],
   "id": "7eebc11dc6d13b56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📥 Load Test Data and Models\n",
    "\n",
    "In this section, we load the preprocessed test data from ERA5 (inputs) and CERRA (targets), and load the trained models stored on disk."
   ],
   "id": "24a8ca7d925d8a23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from source.generate_dataloader import load_netcdf_pair\n",
    "\n",
    "logging.info(\"Generating test dataloader...\")\n",
    "\n",
    "# Paths\n",
    "test_era5 = \"../data/test_era5.nc\"\n",
    "input_lats = xr.open_dataset(\"../data/test_era5.nc\")[\"lat\"].values\n",
    "input_lons = xr.open_dataset(\"../data/test_era5.nc\")[\"lon\"].values\n",
    "\n",
    "test_cerra = \"../data/test_cerra.nc\"\n",
    "output_lats = xr.open_dataset(\"../data/test_cerra.nc\")[\"lat\"].values\n",
    "output_lons = xr.open_dataset(\"../data/test_cerra.nc\")[\"lon\"].values\n",
    "\n",
    "test_dataloader = load_netcdf_pair(test_era5, test_cerra, batch_size=1)\n",
    "input_sample, target_sample = test_dataloader.dataset[0]\n",
    "logging.info(\"Test dataloader created successfully\")\n",
    "logging.info(f\"Loaded one test sample with shape: {input_sample.shape}\")"
   ],
   "id": "c61dcd3fdf291534",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔧 Load Trained DeepESD and U-Net Models\n",
    "\n",
    "Now we initialize both architectures and load their pre-trained weights. These models were trained for temperature downscaling in the WS4 training notebook.\n"
   ],
   "id": "fb0121ee2588a7b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Paths to trained models\n",
    "model_path_deepesd = \"../models/model_deepesd.pt\"\n",
    "model_path_unet = \"../models/model_unet.pt\"\n",
    "\n",
    "# Infer shapes from test data\n",
    "input_shape = input_sample.shape[-2:]\n",
    "output_shape = target_sample.shape[-2:]\n",
    "\n",
    "logging.info(\"🔧 Loading DeepESD model...\")\n",
    "deepesd_model = DeepESD(input_shape, output_shape, 1, 1)\n",
    "deepesd_model.load_state_dict(torch.load(model_path_deepesd, map_location=DEVICE))\n",
    "deepesd_model.to(DEVICE).eval()\n",
    "logging.info(\"✅ DeepESD loaded.\")\n",
    "\n",
    "logging.info(\"🔧 Loading U-Net model...\")\n",
    "unet_model = UNet(input_shape, output_shape, 1, 1)\n",
    "unet_model.load_state_dict(torch.load(model_path_unet, map_location=DEVICE))\n",
    "unet_model.to(DEVICE).eval()\n",
    "logging.info(\"✅ U-Net loaded.\")"
   ],
   "id": "4b56295cf3d986b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🎯 Objective: Understanding Model Behavior at High-Error Locations\n",
    "In this section, we aim to analyze how our deep learning models make predictions for temperature downscaling at spatial points where the models exhibit the highest errors. We focus on two models:\n",
    "\n",
    "DeepESD: a custom deep learning architecture for spatial downscaling,\n",
    "\n",
    "U-Net: a widely used convolutional neural network architecture in climate and image processing.\n",
    "\n",
    "By identifying points where models perform poorly, we can apply Explainable AI (XAI) techniques to better understand why those predictions may be unreliable, inconsistent, or driven by unexpected patterns in the input.\n"
   ],
   "id": "4a3760ed6916b563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load predictions and target data\n",
    "deepesd_ds = xr.open_dataset(\"../data/test_deepesd.nc\")\n",
    "unet_ds = xr.open_dataset(\"../data/test_unet.nc\")\n",
    "target_ds = xr.open_dataset(\"../data/test_cerra.nc\")\n",
    "\n",
    "# Load data\n",
    "deepesd_np = xr.open_dataset(\"../data/test_deepesd.nc\")[\n",
    "    \"t2m\"\n",
    "].values  # (time, lat, lon)\n",
    "unet_np = xr.open_dataset(\"../data/test_unet.nc\")[\"t2m\"].values\n",
    "target_np = xr.open_dataset(\"../data/test_cerra.nc\")[\"t2m\"].values\n",
    "\n",
    "# Ensure all arrays have shape (time, lat, lon)\n",
    "if target_np.shape != deepesd_np.shape:\n",
    "    target_np = np.transpose(target_np, (2, 0, 1))"
   ],
   "id": "50d834dc0f65dada",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📊 How Are the High-Error Pixels Selected?\n",
    "To identify the most relevant locations:\n",
    "\n",
    "1. Compute the RMSE (Root Mean Squared Error) between model predictions and the ground truth (CERRA) across time, for each pixel.\n",
    "\n",
    "2. Select the top 5 pixels with the highest RMSE, but enforce a minimum spatial separation between selected points (e.g. 10 grid cells) to avoid clustering around the same area.\n",
    "\n",
    "3. Map each selected output location (high-resolution CERRA grid) to the nearest input grid cell in ERA5. This is necessary because input and output grids have different spatial resolutions."
   ],
   "id": "9b8bd04ce6dc234"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute RMSE for each pixel (lat, lon)\n",
    "rmse_deepesd = np.sqrt(np.mean((deepesd_np - target_np) ** 2, axis=0))\n",
    "rmse_unet = np.sqrt(np.mean((unet_np - target_np) ** 2, axis=0))\n",
    "\n",
    "\n",
    "def select_diverse_high_error_pixels(error_map, num_pixels=5, min_distance=10):\n",
    "    flat_indices = np.argsort(error_map.ravel())[::-1]\n",
    "    lat_lon_indices = np.array(np.unravel_index(flat_indices, error_map.shape)).T\n",
    "\n",
    "    selected = []\n",
    "    for lat_idx, lon_idx in lat_lon_indices:\n",
    "        if all(\n",
    "            np.linalg.norm(np.array([lat_idx, lon_idx]) - np.array(p)) >= min_distance\n",
    "            for p in selected\n",
    "        ):\n",
    "            selected.append([lat_idx, lon_idx])\n",
    "        if len(selected) == num_pixels:\n",
    "            break\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "# Apply to each RMSE map\n",
    "coords_deepesd = select_diverse_high_error_pixels(\n",
    "    rmse_deepesd, num_pixels=5, min_distance=10\n",
    ")\n",
    "coords_unet = select_diverse_high_error_pixels(rmse_unet, num_pixels=5, min_distance=10)\n",
    "\n",
    "\n",
    "def find_nearest_index(array, value):\n",
    "    return (np.abs(array - value)).argmin()\n",
    "\n",
    "\n",
    "def build_pixel_df(model_name, coords, rmse_map):\n",
    "    output_lat_idx = [lat for lat, lon in coords]\n",
    "    output_lon_idx = [lon for lat, lon in coords]\n",
    "    output_lat_val = [output_lats[lat] for lat in output_lat_idx]\n",
    "    output_lon_val = [output_lons[lon] for lon in output_lon_idx]\n",
    "    input_lat_idx = [find_nearest_index(input_lats, val) for val in output_lat_val]\n",
    "    input_lon_idx = [find_nearest_index(input_lons, val) for val in output_lon_val]\n",
    "    rmse_vals = [rmse_map[lat, lon] for lat, lon in coords]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"Model\": [model_name] * len(coords),\n",
    "            \"RMSE\": rmse_vals,\n",
    "            \"output_lat_idx\": output_lat_idx,\n",
    "            \"output_lon_idx\": output_lon_idx,\n",
    "            \"output_lat_value\": output_lat_val,\n",
    "            \"output_lon_value\": output_lon_val,\n",
    "            \"input_lat_idx\": input_lat_idx,\n",
    "            \"input_lon_idx\": input_lon_idx,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Build separate DataFrames and concatenate\n",
    "df_deepesd = build_pixel_df(\"DeepESD\", coords_deepesd, rmse_deepesd)\n",
    "df_unet = build_pixel_df(\"UNet\", coords_unet, rmse_unet)\n",
    "df = pd.concat([df_deepesd, df_unet], ignore_index=True)\n",
    "\n",
    "display(df)"
   ],
   "id": "e66ddfd798153969",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧠 Explainable AI Techniques Used\n",
    "\n",
    "We use **saliency-based XAI methods** from the [Captum](https://captum.ai) library to explain how the models arrived at their predictions for each selected **high-error output pixel**.\n",
    "\n",
    "The goal is to compute the **attribution of each input pixel** (from ERA5) to the prediction at a specific output location (in CERRA). This highlights **which parts of the input the model considers most influential** for a particular forecast.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Saliency Maps\n",
    "\n",
    "**Saliency maps** compute the gradient of the output with respect to the input. They answer the question:\n",
    "\n",
    "> *How much would a small change in each input pixel affect the prediction at the output pixel of interest?*\n",
    "\n",
    "Key properties:\n",
    "- ✅ Simple and fast to compute.\n",
    "- ✅ Highlights the most influential regions in the input.\n",
    "- ⚠️ Sensitive to noise and input perturbations.\n",
    "- ⚠️ Can suffer from gradient saturation.\n",
    "\n",
    "Mathematically, for an input $x$ and a model $f$:\n",
    "\n",
    "![Saliency Equation](../outputs/figures/equations/saliency.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Integrated Gradients\n",
    "\n",
    "**Integrated Gradients (IG)** aim to provide more stable and reliable attributions by averaging gradients along a straight-line path from a **baseline input** (e.g., a tensor of zeros) to the actual input.\n",
    "\n",
    "IG addresses some of the limitations of raw gradients by integrating over the model’s sensitivity.\n",
    "\n",
    "Formally, for an input $x$, baseline $x'$, and model $f$:\n",
    "\n",
    "![Integrated Gradients Equation](../outputs/figures/equations/integrated_gradients.jpg)\n",
    "\n",
    "Where:\n",
    "- $x$ is the actual input.\n",
    "- $x'$ is the baseline input.\n",
    "- $f(x)$ is the model’s output at the selected output pixel.\n",
    "\n",
    "Key advantages:\n",
    "- ✅ More robust to noise.\n",
    "- ✅ Provides **axiomatic guarantees** (e.g. completeness).\n",
    "- ✅ More faithful to model internals."
   ],
   "id": "bc4016654b44c62e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from captum.attr import Saliency, IntegratedGradients\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Ensure models are in eval mode\n",
    "deepesd_model.eval()\n",
    "unet_model.eval()\n",
    "\n",
    "# Get one batch\n",
    "x_batch, y_batch = test_dataloader.dataset[0]\n",
    "x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "# Extract input-side pixel indices from df\n",
    "deepesd_pixels = df[df[\"Model\"] == \"DeepESD\"][\n",
    "    [\"output_lat_idx\", \"output_lon_idx\"]\n",
    "].values\n",
    "unet_pixels = df[df[\"Model\"] == \"UNet\"][[\"output_lat_idx\", \"output_lon_idx\"]].values\n",
    "\n",
    "# Get spatial dimensions from input batch\n",
    "_, _, H, W = x_batch.shape\n",
    "\n",
    "\n",
    "def compute_attributions(model, x, lat_idx, lon_idx):\n",
    "    x = x.requires_grad_()\n",
    "    lat_idx = min(lat_idx, H - 1)\n",
    "    lon_idx = min(lon_idx, W - 1)\n",
    "\n",
    "    def forward_func(input_tensor):\n",
    "        output = model(input_tensor)\n",
    "        return output[:, 0, lat_idx, lon_idx]  # shape (B,)\n",
    "\n",
    "    model.zero_grad()\n",
    "    saliency = Saliency(forward_func).attribute(x, abs=True)\n",
    "    intgrad = IntegratedGradients(forward_func).attribute(\n",
    "        x, baselines=torch.zeros_like(x)\n",
    "    )\n",
    "\n",
    "    return saliency.detach().cpu().numpy(), intgrad.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# Store results\n",
    "attributions = {\n",
    "    \"DeepESD\": {\"saliency\": [], \"ig\": []},\n",
    "    \"UNet\": {\"saliency\": [], \"ig\": []},\n",
    "}\n",
    "\n",
    "# DeepESD\n",
    "logging.info(\"🔍 Generating attributions for DeepESD...\")\n",
    "for lat_idx, lon_idx in deepesd_pixels:\n",
    "    sal, ig = compute_attributions(deepesd_model, x_batch, lat_idx, lon_idx)\n",
    "    attributions[\"DeepESD\"][\"saliency\"].append(sal)\n",
    "    attributions[\"DeepESD\"][\"ig\"].append(ig)\n",
    "\n",
    "# UNet\n",
    "logging.info(\"🔍 Generating attributions for UNet...\")\n",
    "for lat_idx, lon_idx in unet_pixels:\n",
    "    sal, ig = compute_attributions(unet_model, x_batch, lat_idx, lon_idx)\n",
    "    attributions[\"UNet\"][\"saliency\"].append(sal)\n",
    "    attributions[\"UNet\"][\"ig\"].append(ig)\n",
    "\n",
    "logging.info(\"✅ All saliency and IG maps computed.\")"
   ],
   "id": "6d9eb93adea16a14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"DeepESD saliency:\", len(attributions[\"DeepESD\"][\"saliency\"]))\n",
    "print(\"UNet saliency:\", len(attributions[\"UNet\"][\"saliency\"]))\n",
    "print(\"DeepESD IG:\", len(attributions[\"DeepESD\"][\"ig\"]))\n",
    "print(\"UNet IG:\", len(attributions[\"UNet\"][\"ig\"]))"
   ],
   "id": "ebed177812b55341",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from source.plot_explanation_maps import plot_explanation_map",
   "id": "5fb1c8e955d4b390",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "with open(\"../source/plot_explanation_maps.py\", \"r\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ],
   "id": "166d7d13ae9918e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate and save explanation figures\n",
    "os.makedirs(\"../outputs/figures/explainability\", exist_ok=True)\n",
    "\n",
    "for i in range(5):\n",
    "    output_lat_idx, output_lon_idx = deepesd_pixels[i]\n",
    "    input_coords = df[df[\"output_lat_idx\"] == output_lat_idx][\n",
    "        df[\"output_lon_idx\"] == output_lon_idx\n",
    "    ][[\"input_lat_idx\", \"input_lon_idx\"]].values[0]\n",
    "    input_lat_idx, input_lon_idx = input_coords\n",
    "\n",
    "    output_path = plot_explanation_map(\n",
    "        attributions[\"DeepESD\"][\"saliency\"][i],\n",
    "        title=\"DeepESD - Saliency\",\n",
    "        input_lats=input_lats,\n",
    "        input_lons=input_lons,\n",
    "        filename=f\"../outputs/figures/explainability/deepesd_saliency_pixel-{i+1}.png\",\n",
    "        pixel_coords=(input_lat_idx, input_lon_idx),\n",
    "    )\n",
    "    logging.info(f\"📸 Figure saved to {output_path}\")\n",
    "    display(Image(output_path))\n",
    "\n",
    "    output_path = plot_explanation_map(\n",
    "        attributions[\"DeepESD\"][\"ig\"][i],\n",
    "        title=\"DeepESD - IntegratedGradients\",\n",
    "        input_lats=input_lats,\n",
    "        input_lons=input_lons,\n",
    "        filename=f\"../outputs/figures/explainability/deepesd_ig_pixel-{i+1}.png\",\n",
    "        pixel_coords=(input_lat_idx, input_lon_idx),\n",
    "    )\n",
    "    logging.info(f\"📸 Figure saved to {output_path}\")\n",
    "    display(Image(output_path))\n",
    "\n",
    "for i in range(5):\n",
    "    output_lat_idx, output_lon_idx = unet_pixels[i]\n",
    "    input_coords = df[df[\"output_lat_idx\"] == output_lat_idx][\n",
    "        df[\"output_lon_idx\"] == output_lon_idx\n",
    "    ][[\"input_lat_idx\", \"input_lon_idx\"]].values[0]\n",
    "    input_lat_idx, input_lon_idx = input_coords\n",
    "\n",
    "    output_path = plot_explanation_map(\n",
    "        attributions[\"UNet\"][\"saliency\"][i],\n",
    "        title=\"UNet - Saliency\",\n",
    "        input_lats=input_lats,\n",
    "        input_lons=input_lons,\n",
    "        filename=f\"../outputs/figures/explainability/unet_saliency_pixel-{i+1}.png\",\n",
    "        pixel_coords=(input_lat_idx, input_lon_idx),\n",
    "    )\n",
    "    logging.info(f\"📸 Figure saved to {output_path}\")\n",
    "    display(Image(output_path))\n",
    "\n",
    "    output_path = plot_explanation_map(\n",
    "        attributions[\"UNet\"][\"ig\"][i],\n",
    "        title=\"UNet - IntegratedGradients\",\n",
    "        input_lats=input_lats,\n",
    "        input_lons=input_lons,\n",
    "        filename=f\"../outputs/figures/explainability/unet_ig_pixel-{i+1}.png\",\n",
    "        pixel_coords=(input_lat_idx, input_lon_idx),\n",
    "    )\n",
    "    logging.info(f\"📸 Figure saved to {output_path}\")\n",
    "    display(Image(output_path))"
   ],
   "id": "c18f62d031fa1656",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
